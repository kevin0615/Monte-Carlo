{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib.pyplot import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import standard_normal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browmian motion class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class European_Call_Payoff:\n",
    "\n",
    "    def __init__(self, strike):\n",
    "        self.strike = strike\n",
    "\n",
    "    def get_payoff(self, stock_price):\n",
    "        if stock_price > self.strike:\n",
    "            return stock_price - self.strike\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "class GeometricBrownianMotion:\n",
    "\n",
    "    def simulate_paths(self):\n",
    "        while(self.T - self.dt > 0):\n",
    "            dWt = np.random.normal(0, 1)  # Brownian motion\n",
    "            dYt = 1 + self.drift*self.dt + self.volatility*math.sqrt(self.dt)*dWt  # Change in price\n",
    "            self.current_price = dYt*self.current_price # Add the change to the current price\n",
    "            self.prices.append(self.current_price)  # Append new price to series\n",
    "            self.T -= self.dt  # Accound for the step in time \n",
    "\n",
    "    def __init__(self, initial_price, drift, volatility, dt, T):\n",
    "        self.current_price = initial_price\n",
    "        self.initial_price = initial_price\n",
    "        self.drift = drift\n",
    "        self.volatility = volatility\n",
    "        self.dt = dt\n",
    "        self.T = T\n",
    "        self.prices = []\n",
    "        self.simulate_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heston pricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HestonPricer:\n",
    "\n",
    "    def simulate_paths(self):\n",
    "        while(self.T - self.dt > 0):\n",
    "            \n",
    "            # Normal distribution volatility\n",
    "            epsilonV = np.random.normal(0, 1) \n",
    "            #compute volatility\n",
    "            \n",
    "             # Normal distribution for the price\n",
    "            epsilonS = np.random.normal(0, 1)  \n",
    "            \n",
    "            self.current_price = self.current_price*(1 + self.mu*self.dt + \n",
    "                                                      math.sqrt(self.current_volatility*self.dt)*epsilonS)\n",
    "            self.current_volatility= np.abs((self.current_volatility \n",
    "                                     +self.k*(self.teta-self.current_volatility)*self.dt \n",
    "                                     +self.volatility_std*(math.sqrt(self.current_volatility*self.dt))*epsilonV))\n",
    "                    \n",
    "            self.prices.append(self.current_price)  # Append new price to series\n",
    "            self.T -= self.dt  # Accound for the step in time \n",
    "\n",
    "    def __init__(self, mu, teta, k, volatility_std, initial_price, initial_volatility, dt, T):\n",
    "        \n",
    "        #heston parameters \n",
    "        self.mu = mu\n",
    "        self.teta = teta\n",
    "        self.k = k\n",
    "        self.volatility_std= volatility_std\n",
    "        #initialization parameters\n",
    "        self.current_price = initial_price\n",
    "        self.initial_price = initial_price\n",
    "        self.initial_volatility = initial_volatility\n",
    "        self.current_volatility = initial_volatility\n",
    "       \n",
    "        self.dt = dt\n",
    "        self.T = T\n",
    "        self.prices = []\n",
    "        self.simulate_paths()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte carlo function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monteCarlo(paths = [], spot_prices = [], interest_rates = [], volatilities = [], expiry_times = [],\n",
    "               strike_prices = [], drift = 0.1, dt = 1/252, mu = 0.1, \n",
    "               teta = 0.1, k = 0.1, volatility_std = 0.01, flag=1):\n",
    "    data= []\n",
    "    np.random.seed(32)\n",
    "    for sigma in tqdm(volatilities):\n",
    "\n",
    "        for r in interest_rates:\n",
    "\n",
    "            for T in expiry_times:\n",
    "\n",
    "                for S0 in spot_prices:\n",
    "\n",
    "                    for K in strike_prices:\n",
    "\n",
    "                        # Monte Carlo simulation\n",
    "                        price_paths=[] \n",
    "\n",
    "                        for i in range(0, paths):\n",
    "                            if flag == 1:\n",
    "                                price_paths.append(GeometricBrownianMotion(S0, drift, sigma, dt, T/252).prices)\n",
    "                                # price = analytical_call_pricer(r, sigma, S0, T,K,option_type)\n",
    "                            if flag == 2:\n",
    "                                price_paths.append(HestonPricer(mu, teta, k, volatility_std, \n",
    "                                                                S0, sigma, dt, T/252).prices)\n",
    "\n",
    "                        call_payoffs = []\n",
    "                        ec = European_Call_Payoff(K)\n",
    "\n",
    "                        for price_path in price_paths:\n",
    "                            # We get the last stock price in the series generated by GBM to determin the payoff and discount it by one year\n",
    "                            call_payoffs.append(ec.get_payoff(price_path[-1])/((1 + r)**(T/252)))  \n",
    "                        #adding data to our dataset\n",
    "                        data.append({\n",
    "                            'volatility':sigma,\n",
    "                            'interest_rate':r,\n",
    "                            'expiry_time':T,\n",
    "                            'initial_price':S0,\n",
    "                            'strike_price':K,\n",
    "                            'price':np.average(call_payoffs)\n",
    "                        })\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.head()\n",
    "    X=df[['volatility','interest_rate','expiry_time','initial_price','strike_price']].values\n",
    "    y= df[['price']].values\n",
    "    X_train, X_val, Y_train, y_val= train_test_split(X, y, test_size=0.20)\n",
    "    return X_train, X_val, Y_train, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black Scholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:52<00:00, 18.72s/it]\n"
     ]
    }
   ],
   "source": [
    "paths=10000\n",
    "spot_prices = [v for v in range(5,100,20)]\n",
    "interest_rates= [0.01, 0.015, 0.05, 0.043]\n",
    "volatilities = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "expiry_times = [7, 14, 21, 28, 35, 42, 49]\n",
    "strike_prices = [v for v in range(30,70,10)]\n",
    "drift = .08\n",
    "dt = 1/252\n",
    "\n",
    "X_train, X_val, Y_train, y_val = monteCarlo(paths = paths, spot_prices = spot_prices, interest_rates = interest_rates,\n",
    "                                            volatilities = volatilities, expiry_times = expiry_times, \n",
    "                                            strike_prices = strike_prices, drift = drift, dt= dt, flag = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2688, 5) (2688, 1)\n",
      "(672, 5) (672, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Scholes - Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=5, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy', 'mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 4s 23ms/step - loss: 516.0301 - accuracy: 0.4295 - mse: 516.0301 - mae: 14.1773 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 481.1510 - accuracy: 0.4460 - mse: 481.1510 - mae: 13.2740 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 501.8728 - accuracy: 0.4449 - mse: 501.8728 - mae: 13.7433 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 470.8484 - accuracy: 0.4289 - mse: 470.8484 - mae: 13.2897 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 461.3775 - accuracy: 0.4471 - mse: 461.3775 - mae: 13.1364 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 479.3154 - accuracy: 0.4414 - mse: 479.3154 - mae: 13.3521 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 1s 35ms/step - loss: 478.7557 - accuracy: 0.4474 - mse: 478.7557 - mae: 13.3474 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 483.0834 - accuracy: 0.4502 - mse: 483.0834 - mae: 13.3772 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 497.9799 - accuracy: 0.4565 - mse: 497.9799 - mae: 13.5361 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 477.2607 - accuracy: 0.4397 - mse: 477.2607 - mae: 13.3988 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 497.6919 - accuracy: 0.4471 - mse: 497.6919 - mae: 13.6262 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 473.2856 - accuracy: 0.4570 - mse: 473.2856 - mae: 13.2076 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 483.2270 - accuracy: 0.4504 - mse: 483.2270 - mae: 13.5052 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 515.6035 - accuracy: 0.4385 - mse: 515.6035 - mae: 14.0675 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 523.1141 - accuracy: 0.4412 - mse: 523.1141 - mae: 13.8946 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 503.1346 - accuracy: 0.4551 - mse: 503.1346 - mae: 13.7149 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 453.1772 - accuracy: 0.4676 - mse: 453.1772 - mae: 12.6126 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 475.6770 - accuracy: 0.4539 - mse: 475.6770 - mae: 13.2087 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 463.8369 - accuracy: 0.4544 - mse: 463.8369 - mae: 12.9939 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 471.2408 - accuracy: 0.4582 - mse: 471.2408 - mae: 13.1437 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 463.7607 - accuracy: 0.4586 - mse: 463.7607 - mae: 12.9032 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 494.5525 - accuracy: 0.4572 - mse: 494.5525 - mae: 13.5978 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 477.3851 - accuracy: 0.4591 - mse: 477.3851 - mae: 13.1350 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 463.6247 - accuracy: 0.4538 - mse: 463.6247 - mae: 12.9685 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 503.2169 - accuracy: 0.4550 - mse: 503.2169 - mae: 13.6091 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 483.5562 - accuracy: 0.4641 - mse: 483.5562 - mae: 13.2252 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 476.0593 - accuracy: 0.4560 - mse: 476.0593 - mae: 13.2190 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 498.8820 - accuracy: 0.4403 - mse: 498.8820 - mae: 13.7500 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 501.1115 - accuracy: 0.4463 - mse: 501.1115 - mae: 13.7730 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 474.8381 - accuracy: 0.4611 - mse: 474.8381 - mae: 13.1993 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 496.9238 - accuracy: 0.4494 - mse: 496.9238 - mae: 13.5445 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 460.2798 - accuracy: 0.4687 - mse: 460.2798 - mae: 12.7232 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 474.5703 - accuracy: 0.4471 - mse: 474.5703 - mae: 13.3154 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 453.2253 - accuracy: 0.4823 - mse: 453.2253 - mae: 12.6170 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 452.9362 - accuracy: 0.4480 - mse: 452.9362 - mae: 12.8702 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 496.5733 - accuracy: 0.4490 - mse: 496.5733 - mae: 13.5970 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 479.3427 - accuracy: 0.4625 - mse: 479.3427 - mae: 13.1786 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 510.5681 - accuracy: 0.4398 - mse: 510.5681 - mae: 13.8904 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 501.5487 - accuracy: 0.4546 - mse: 501.5487 - mae: 13.6910 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 515.0174 - accuracy: 0.4436 - mse: 515.0174 - mae: 13.8981 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 478.7376 - accuracy: 0.4456 - mse: 478.7376 - mae: 13.4006 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 491.4707 - accuracy: 0.4616 - mse: 491.4707 - mae: 13.4004 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 487.4551 - accuracy: 0.4600 - mse: 487.4551 - mae: 13.2845 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 491.5622 - accuracy: 0.4600 - mse: 491.5622 - mae: 13.5752 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 475.6313 - accuracy: 0.4572 - mse: 475.6313 - mae: 13.2599 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 471.9523 - accuracy: 0.4580 - mse: 471.9523 - mae: 13.0851 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 476.4925 - accuracy: 0.4550 - mse: 476.4925 - mae: 13.1423 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 475.6257 - accuracy: 0.4520 - mse: 475.6257 - mae: 13.2282 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 488.7255 - accuracy: 0.4489 - mse: 488.7255 - mae: 13.4624 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 481.5790 - accuracy: 0.4463 - mse: 481.5790 - mae: 13.4768 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 485.4776 - accuracy: 0.4537 - mse: 485.4776 - mae: 13.3316 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 482.6728 - accuracy: 0.4589 - mse: 482.6728 - mae: 13.3690 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 480.2534 - accuracy: 0.4613 - mse: 480.2534 - mae: 13.2351 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 464.2940 - accuracy: 0.4656 - mse: 464.2940 - mae: 12.9207 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 488.4870 - accuracy: 0.4521 - mse: 488.4870 - mae: 13.3897 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 470.5643 - accuracy: 0.4547 - mse: 470.5643 - mae: 12.9643 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 457.5590 - accuracy: 0.4767 - mse: 457.5590 - mae: 12.7085 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 487.7744 - accuracy: 0.4743 - mse: 487.7744 - mae: 13.2044 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 487.8945 - accuracy: 0.4498 - mse: 487.8945 - mae: 13.4068 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 476.3360 - accuracy: 0.4618 - mse: 476.3360 - mae: 13.1046 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 459.8085 - accuracy: 0.4664 - mse: 459.8085 - mae: 12.7674 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 487.2430 - accuracy: 0.4401 - mse: 487.2430 - mae: 13.4760 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 475.1303 - accuracy: 0.4611 - mse: 475.1303 - mae: 13.1730 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 511.2941 - accuracy: 0.4413 - mse: 511.2941 - mae: 14.0019 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 493.5495 - accuracy: 0.4474 - mse: 493.5495 - mae: 13.6293 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 476.2960 - accuracy: 0.4563 - mse: 476.2960 - mae: 13.1873 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 452.6770 - accuracy: 0.4620 - mse: 452.6770 - mae: 12.6799 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 482.3881 - accuracy: 0.4665 - mse: 482.3881 - mae: 13.1259 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 451.2973 - accuracy: 0.4695 - mse: 451.2973 - mae: 12.7048 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 471.8592 - accuracy: 0.4691 - mse: 471.8592 - mae: 13.1407 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 1s 19ms/step - loss: 482.1396 - accuracy: 0.4480 - mse: 482.1396 - mae: 13.2815 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 503.6716 - accuracy: 0.4439 - mse: 503.6716 - mae: 13.8648 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 470.5643 - accuracy: 0.4567 - mse: 470.5643 - mae: 13.1058 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 461.5909 - accuracy: 0.4631 - mse: 461.5909 - mae: 12.7573 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 491.2470 - accuracy: 0.4533 - mse: 491.2470 - mae: 13.4686 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 483.6127 - accuracy: 0.4535 - mse: 483.6127 - mae: 13.2411 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 484.3365 - accuracy: 0.4504 - mse: 484.3365 - mae: 13.3639 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 488.0854 - accuracy: 0.4548 - mse: 488.0854 - mae: 13.2987 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 449.4835 - accuracy: 0.4540 - mse: 449.4835 - mae: 12.7637 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 491.5038 - accuracy: 0.4612 - mse: 491.5038 - mae: 13.4319 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 468.9430 - accuracy: 0.4526 - mse: 468.9430 - mae: 13.0262 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 477.9124 - accuracy: 0.4488 - mse: 477.9124 - mae: 13.3524 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 486.2384 - accuracy: 0.4495 - mse: 486.2384 - mae: 13.3407 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 484.4872 - accuracy: 0.4622 - mse: 484.4872 - mae: 13.2379 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 450.4244 - accuracy: 0.4662 - mse: 450.4244 - mae: 12.7550 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 489.6200 - accuracy: 0.4460 - mse: 489.6200 - mae: 13.6425 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 486.2182 - accuracy: 0.4780 - mse: 486.2182 - mae: 13.2962 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 477.7997 - accuracy: 0.4473 - mse: 477.7997 - mae: 13.2593 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 469.6057 - accuracy: 0.4500 - mse: 469.6057 - mae: 13.1389 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 498.8556 - accuracy: 0.4539 - mse: 498.8556 - mae: 13.5660 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 483.5086 - accuracy: 0.4587 - mse: 483.5086 - mae: 13.3206 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 452.6770 - accuracy: 0.4637 - mse: 452.6770 - mae: 12.5835 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 493.1565 - accuracy: 0.4487 - mse: 493.1565 - mae: 13.4916 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 468.3762 - accuracy: 0.4541 - mse: 468.3762 - mae: 13.1915 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 469.9621 - accuracy: 0.4577 - mse: 469.9621 - mae: 13.1874 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 485.5464 - accuracy: 0.4492 - mse: 485.5464 - mae: 13.5309 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 477.8154 - accuracy: 0.4592 - mse: 477.8154 - mae: 13.1883 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 459.9238 - accuracy: 0.4638 - mse: 459.9238 - mae: 12.8184 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 477.4947 - accuracy: 0.4625 - mse: 477.4947 - mae: 13.0904 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 487.2728 - accuracy: 0.4497 - mse: 487.2728 - mae: 13.4222 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 506.9957 - accuracy: 0.4428 - mse: 506.9957 - mae: 13.8436 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 465.7335 - accuracy: 0.4645 - mse: 465.7335 - mae: 12.9954 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 490.9800 - accuracy: 0.4410 - mse: 490.9800 - mae: 13.5117 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 493.8189 - accuracy: 0.4408 - mse: 493.8189 - mae: 13.6822 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 481.3974 - accuracy: 0.4456 - mse: 481.3974 - mae: 13.4878 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 484.1160 - accuracy: 0.4566 - mse: 484.1160 - mae: 13.3699 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 492.6214 - accuracy: 0.4333 - mse: 492.6214 - mae: 13.7333 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 491.3356 - accuracy: 0.4572 - mse: 491.3356 - mae: 13.4096 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 483.7928 - accuracy: 0.4556 - mse: 483.7928 - mae: 13.3649 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 446.8023 - accuracy: 0.4690 - mse: 446.8023 - mae: 12.6486 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 477.0299 - accuracy: 0.4541 - mse: 477.0299 - mae: 13.2410 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 506.5100 - accuracy: 0.4458 - mse: 506.5100 - mae: 13.8519 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 491.7664 - accuracy: 0.4341 - mse: 491.7664 - mae: 13.6898 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 485.2610 - accuracy: 0.4683 - mse: 485.2610 - mae: 13.1702 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 481.4882 - accuracy: 0.4581 - mse: 481.4882 - mae: 13.3480 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 469.2379 - accuracy: 0.4700 - mse: 469.2379 - mae: 12.9823 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 492.0246 - accuracy: 0.4423 - mse: 492.0246 - mae: 13.7097 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 478.7697 - accuracy: 0.4604 - mse: 478.7697 - mae: 13.1800 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 494.5660 - accuracy: 0.4401 - mse: 494.5660 - mae: 13.6789 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 463.0783 - accuracy: 0.4568 - mse: 463.0783 - mae: 12.9580 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 471.7950 - accuracy: 0.4575 - mse: 471.7950 - mae: 13.1187 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 472.1059 - accuracy: 0.4435 - mse: 472.1059 - mae: 13.2543 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 483.8816 - accuracy: 0.4687 - mse: 483.8816 - mae: 13.1779 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 476.2268 - accuracy: 0.4605 - mse: 476.2268 - mae: 13.1528 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 490.7182 - accuracy: 0.4594 - mse: 490.7182 - mae: 13.3920 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 468.9188 - accuracy: 0.4570 - mse: 468.9188 - mae: 12.9818 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 478.9459 - accuracy: 0.4422 - mse: 478.9459 - mae: 13.4284 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 484.5400 - accuracy: 0.4639 - mse: 484.5400 - mae: 13.3193 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 485.8533 - accuracy: 0.4480 - mse: 485.8533 - mae: 13.4082 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 462.6198 - accuracy: 0.4521 - mse: 462.6198 - mae: 13.0340 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 489.0302 - accuracy: 0.4546 - mse: 489.0302 - mae: 13.4519 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 489.3334 - accuracy: 0.4503 - mse: 489.3334 - mae: 13.5145 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 471.3179 - accuracy: 0.4433 - mse: 471.3179 - mae: 13.2686 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 498.5802 - accuracy: 0.4352 - mse: 498.5802 - mae: 13.7956 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 484.1946 - accuracy: 0.4472 - mse: 484.1946 - mae: 13.3942 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 474.7592 - accuracy: 0.4638 - mse: 474.7592 - mae: 13.2143 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 453.3851 - accuracy: 0.4631 - mse: 453.3851 - mae: 12.8037 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 469.0699 - accuracy: 0.4532 - mse: 469.0699 - mae: 13.1102 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 499.9113 - accuracy: 0.4374 - mse: 499.9113 - mae: 13.8222 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 484.2668 - accuracy: 0.4680 - mse: 484.2668 - mae: 13.2112 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 1s 17ms/step - loss: 480.3309 - accuracy: 0.4597 - mse: 480.3309 - mae: 13.2527 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 489.9354 - accuracy: 0.4520 - mse: 489.9354 - mae: 13.3997 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 457.9310 - accuracy: 0.4539 - mse: 457.9310 - mae: 12.9402 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 483.5560 - accuracy: 0.4523 - mse: 483.5560 - mae: 13.4419 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 496.8088 - accuracy: 0.4424 - mse: 496.8088 - mae: 13.6804 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 492.5075 - accuracy: 0.4464 - mse: 492.5075 - mae: 13.5487 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 498.5073 - accuracy: 0.4376 - mse: 498.5073 - mae: 13.7270 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 486.9310 - accuracy: 0.4477 - mse: 486.9310 - mae: 13.5732 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 480.5536 - accuracy: 0.4538 - mse: 480.5536 - mae: 13.3174 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 490.0982 - accuracy: 0.4443 - mse: 490.0982 - mae: 13.5682 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 486.6955 - accuracy: 0.4533 - mse: 486.6955 - mae: 13.3098 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 493.6827 - accuracy: 0.4467 - mse: 493.6827 - mae: 13.4697 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 496.8359 - accuracy: 0.4470 - mse: 496.8359 - mae: 13.6475 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 471.6618 - accuracy: 0.4597 - mse: 471.6618 - mae: 13.1248 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 470.7854 - accuracy: 0.4628 - mse: 470.7854 - mae: 13.0301 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 502.8195 - accuracy: 0.4499 - mse: 502.8195 - mae: 13.6883 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 450.7279 - accuracy: 0.4748 - mse: 450.7279 - mae: 12.6102 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 466.0379 - accuracy: 0.4587 - mse: 466.0379 - mae: 13.0323 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 481.0294 - accuracy: 0.4517 - mse: 481.0294 - mae: 13.2999 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 496.4502 - accuracy: 0.4368 - mse: 496.4502 - mae: 13.8278 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 484.7402 - accuracy: 0.4567 - mse: 484.7402 - mae: 13.3609 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 497.0860 - accuracy: 0.4422 - mse: 497.0860 - mae: 13.7688 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 490.8222 - accuracy: 0.4487 - mse: 490.8222 - mae: 13.5335 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 503.8108 - accuracy: 0.4477 - mse: 503.8108 - mae: 13.8834 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 469.5418 - accuracy: 0.4568 - mse: 469.5418 - mae: 13.1063 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 450.7164 - accuracy: 0.4630 - mse: 450.7164 - mae: 12.6838 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 531.5819 - accuracy: 0.4413 - mse: 531.5819 - mae: 14.333 - 0s 8ms/step - loss: 519.3517 - accuracy: 0.4444 - mse: 519.3517 - mae: 14.0778 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 474.8370 - accuracy: 0.4543 - mse: 474.8370 - mae: 13.2483 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 448.9435 - accuracy: 0.4584 - mse: 448.9435 - mae: 12.7384 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 485.4968 - accuracy: 0.4597 - mse: 485.4968 - mae: 13.4131 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 483.3973 - accuracy: 0.4449 - mse: 483.3973 - mae: 13.3727 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 487.9311 - accuracy: 0.4553 - mse: 487.9311 - mae: 13.3957 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 480.6572 - accuracy: 0.4321 - mse: 480.6572 - mae: 13.5426 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 466.2708 - accuracy: 0.4487 - mse: 466.2708 - mae: 13.1172 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 487.8053 - accuracy: 0.4426 - mse: 487.8053 - mae: 13.5456 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 506.7153 - accuracy: 0.4432 - mse: 506.7153 - mae: 13.7664 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 488.9170 - accuracy: 0.4529 - mse: 488.9170 - mae: 13.4174 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 498.8058 - accuracy: 0.4501 - mse: 498.8058 - mae: 13.8014 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 482.4168 - accuracy: 0.4623 - mse: 482.4168 - mae: 13.1962 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 510.1207 - accuracy: 0.4442 - mse: 510.1207 - mae: 13.7857 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 497.5360 - accuracy: 0.4550 - mse: 497.5360 - mae: 13.5965 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 456.3158 - accuracy: 0.4671 - mse: 456.3158 - mae: 12.7570 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 457.3816 - accuracy: 0.4611 - mse: 457.3816 - mae: 12.7622 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 467.4271 - accuracy: 0.4506 - mse: 467.4271 - mae: 13.0931 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 470.6906 - accuracy: 0.4559 - mse: 470.6906 - mae: 13.1176 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 449.0296 - accuracy: 0.4619 - mse: 449.0296 - mae: 12.6543 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 443.5508 - accuracy: 0.4570 - mse: 443.5508 - mae: 12.6386 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 470.3621 - accuracy: 0.4574 - mse: 470.3621 - mae: 12.9384 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 472.9524 - accuracy: 0.4439 - mse: 472.9524 - mae: 13.2916 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 9ms/step - loss: 462.8335 - accuracy: 0.4670 - mse: 462.8335 - mae: 12.8413 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 500.0308 - accuracy: 0.4571 - mse: 500.0308 - mae: 13.7270 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 478.2302 - accuracy: 0.4515 - mse: 478.2302 - mae: 13.2444 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 454.3096 - accuracy: 0.4686 - mse: 454.3096 - mae: 12.7192 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 519.9239 - accuracy: 0.4434 - mse: 519.9239 - mae: 14.0788 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 476.3148 - accuracy: 0.4572 - mse: 476.3148 - mae: 13.2165 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 481.7066 - accuracy: 0.4516 - mse: 481.7066 - mae: 13.4213 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 473.6173 - accuracy: 0.4578 - mse: 473.6173 - mae: 13.1546 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 483.6386 - accuracy: 0.4539 - mse: 483.6386 - mae: 13.4168 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 465.7127 - accuracy: 0.4698 - mse: 465.7127 - mae: 12.9489 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 482.1681 - accuracy: 0.4555 - mse: 482.1681 - mae: 13.2590 - val_loss: 466.4959 - val_accuracy: 0.4405 - val_mse: 466.4959 - val_mae: 13.2376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x126897b1310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=200, batch_size=64, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:38<00:00, 26.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# testing the neural network\n",
    "# Generating a test dataset\n",
    "\n",
    "nb_sample=1000\n",
    "data= []\n",
    "for n in tqdm(range(nb_sample)):\n",
    "    sigma= np.random.uniform(low=volatilities[0],high=volatilities[-1])\n",
    "    r = np.random.uniform(low=interest_rates[0],high=interest_rates[-1])\n",
    "    T= np.random.uniform(low=expiry_times[0],high=expiry_times[-1])\n",
    "    S0= np.random.uniform(low=spot_prices[0],high=spot_prices[-1])\n",
    "    K= np.random.uniform(low=(1-0.2)*S0,high=(1+0.2)*S0)\n",
    "    price_paths=[] \n",
    "    for i in range(0, paths):\n",
    "        price_paths.append(GeometricBrownianMotion(S0, drift, sigma, dt, T/252).prices)\n",
    "    #price = analytical_call_pricer(r, sigma, S0, T,K,option_type)\n",
    "    call_payoffs = []\n",
    "    ec = European_Call_Payoff(K)\n",
    "\n",
    "    for price_path in price_paths:\n",
    "        call_payoffs.append(ec.get_payoff(price_path[-1])/((1 + r)**(T/252)))  # We get the last stock price in the series generated by GBM to determin the payoff and discount it by one year \n",
    "    data.append({\n",
    "                        'volatility':sigma,\n",
    "                        'interest_rate':r,\n",
    "                        'expiry_time':T,\n",
    "                        'initial_price':S0,\n",
    "                        'strike_price':K,\n",
    "                        'price':np.average(call_payoffs)\n",
    "                    })\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()\n",
    "X_test=df[['volatility','interest_rate','expiry_time','initial_price','strike_price']].values\n",
    "y_test= df[['price']].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 17.6800 - accuracy: 0.1320 - mse: 17.6800 - mae: 2.5117\n",
      "test loss: 17.67999267578125\n"
     ]
    }
   ],
   "source": [
    "loss, _, _, _ = model.evaluate(X_test, y_test)\n",
    "print(\"test loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heston - Neural network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [03:37<00:00, 36.27s/it]\n"
     ]
    }
   ],
   "source": [
    "mu = 0.1\n",
    "teta = 0.04\n",
    "k = 2\n",
    "volatility_std = 0.3\n",
    "X_train, X_val, Y_train, y_val = monteCarlo(paths = paths, spot_prices = spot_prices, interest_rates = interest_rates,\n",
    "                                            volatilities = volatilities, expiry_times = expiry_times, \n",
    "                                            strike_prices = strike_prices, dt= dt, mu = mu, \n",
    "                                            teta = teta, k = k, volatility_std = volatility_std, flag=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 2s 21ms/step - loss: 139.8159 - accuracy: 0.1522 - val_loss: 2.3629 - val_accuracy: 0.3676\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 1.2842 - accuracy: 0.3873 - val_loss: 0.6753 - val_accuracy: 0.3676\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.6737 - accuracy: 0.3783 - val_loss: 0.7561 - val_accuracy: 0.3676\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.6195 - accuracy: 0.3937 - val_loss: 0.5765 - val_accuracy: 0.3676\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.5495 - accuracy: 0.3820 - val_loss: 0.7014 - val_accuracy: 0.3676\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.5587 - accuracy: 0.4018 - val_loss: 0.7440 - val_accuracy: 0.3676\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.6020 - accuracy: 0.3655 - val_loss: 0.5676 - val_accuracy: 0.3676\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.6721 - accuracy: 0.3851 - val_loss: 0.5842 - val_accuracy: 0.3676\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.5931 - accuracy: 0.3623 - val_loss: 0.9691 - val_accuracy: 0.3676\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.7176 - accuracy: 0.3706 - val_loss: 0.6231 - val_accuracy: 0.3676\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5513 - accuracy: 0.3801 - val_loss: 0.6049 - val_accuracy: 0.3676\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5517 - accuracy: 0.3779 - val_loss: 0.6363 - val_accuracy: 0.3676\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5938 - accuracy: 0.3755 - val_loss: 0.5969 - val_accuracy: 0.3676\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.6675 - accuracy: 0.3888 - val_loss: 0.6213 - val_accuracy: 0.3676\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.6606 - accuracy: 0.3803 - val_loss: 1.0019 - val_accuracy: 0.3676\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5842 - accuracy: 0.3846 - val_loss: 0.5744 - val_accuracy: 0.3676\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.5544 - accuracy: 0.3890 - val_loss: 0.5892 - val_accuracy: 0.3676\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5705 - accuracy: 0.3748 - val_loss: 1.1552 - val_accuracy: 0.3676\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.7257 - accuracy: 0.3736 - val_loss: 0.6180 - val_accuracy: 0.3676\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.6136 - accuracy: 0.3910 - val_loss: 0.8299 - val_accuracy: 0.3676\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.8353 - accuracy: 0.3772 - val_loss: 1.6238 - val_accuracy: 0.3676\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.7169 - accuracy: 0.3663 - val_loss: 0.6121 - val_accuracy: 0.3676\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.6261 - accuracy: 0.3555 - val_loss: 0.8763 - val_accuracy: 0.3676\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.5506 - accuracy: 0.3783 - val_loss: 1.5775 - val_accuracy: 0.3676\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 1.1956 - accuracy: 0.3720 - val_loss: 0.8541 - val_accuracy: 0.3676\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.0482 - accuracy: 0.3731 - val_loss: 0.6049 - val_accuracy: 0.3676\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.4960 - accuracy: 0.3830 - val_loss: 0.8158 - val_accuracy: 0.3676\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6059 - accuracy: 0.3832 - val_loss: 0.6378 - val_accuracy: 0.3676\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.6581 - accuracy: 0.3763 - val_loss: 0.7830 - val_accuracy: 0.3676\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6090 - accuracy: 0.37 - 0s 8ms/step - loss: 0.6094 - accuracy: 0.3711 - val_loss: 0.7070 - val_accuracy: 0.3676\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6322 - accuracy: 0.3901 - val_loss: 0.5966 - val_accuracy: 0.3676\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5261 - accuracy: 0.3842 - val_loss: 0.8002 - val_accuracy: 0.3676\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6145 - accuracy: 0.3810 - val_loss: 0.7505 - val_accuracy: 0.3676\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6120 - accuracy: 0.3708 - val_loss: 0.6475 - val_accuracy: 0.3676\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5076 - accuracy: 0.3725 - val_loss: 0.6199 - val_accuracy: 0.3676\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5607 - accuracy: 0.3754 - val_loss: 0.9140 - val_accuracy: 0.3676\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.6994 - accuracy: 0.3672 - val_loss: 0.6182 - val_accuracy: 0.3676\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5974 - accuracy: 0.3702 - val_loss: 0.6252 - val_accuracy: 0.3676\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6472 - accuracy: 0.3827 - val_loss: 0.6238 - val_accuracy: 0.3676\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5470 - accuracy: 0.3865 - val_loss: 1.0157 - val_accuracy: 0.3676\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.7182 - accuracy: 0.3866 - val_loss: 0.5885 - val_accuracy: 0.3676\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.5618 - accuracy: 0.3925 - val_loss: 0.5907 - val_accuracy: 0.3676\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.6757 - accuracy: 0.3930 - val_loss: 0.7083 - val_accuracy: 0.3676\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 0.5364 - accuracy: 0.3841 - val_loss: 0.7452 - val_accuracy: 0.3676\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 1s 16ms/step - loss: 0.5641 - accuracy: 0.3802 - val_loss: 0.6781 - val_accuracy: 0.3676\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.5461 - accuracy: 0.3837 - val_loss: 0.8790 - val_accuracy: 0.3676\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.6959 - accuracy: 0.3923 - val_loss: 0.5733 - val_accuracy: 0.3676\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.5355 - accuracy: 0.3812 - val_loss: 0.7092 - val_accuracy: 0.3676\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.7857 - accuracy: 0.3698 - val_loss: 0.9453 - val_accuracy: 0.3676\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6282 - accuracy: 0.3708 - val_loss: 0.5915 - val_accuracy: 0.3676\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.6784 - accuracy: 0.3825 - val_loss: 0.6008 - val_accuracy: 0.3676\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.5040 - accuracy: 0.3800 - val_loss: 0.8218 - val_accuracy: 0.3676\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.6555 - accuracy: 0.3855 - val_loss: 0.7034 - val_accuracy: 0.3676\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.6741 - accuracy: 0.3900 - val_loss: 0.5600 - val_accuracy: 0.3676\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.5080 - accuracy: 0.3885 - val_loss: 0.5383 - val_accuracy: 0.3676\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.4923 - accuracy: 0.3722 - val_loss: 0.5526 - val_accuracy: 0.3676\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.5251 - accuracy: 0.3787 - val_loss: 0.5627 - val_accuracy: 0.3676\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 9ms/step - loss: 0.6763 - accuracy: 0.3710 - val_loss: 1.1178 - val_accuracy: 0.3676\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.8302 - accuracy: 0.3677 - val_loss: 1.0662 - val_accuracy: 0.3676\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.7462 - accuracy: 0.3922 - val_loss: 0.7774 - val_accuracy: 0.3676\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6073 - accuracy: 0.3752 - val_loss: 0.8672 - val_accuracy: 0.3676\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5605 - accuracy: 0.3799 - val_loss: 0.5548 - val_accuracy: 0.3676\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4881 - accuracy: 0.3792 - val_loss: 0.5457 - val_accuracy: 0.3676\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4441 - accuracy: 0.3806 - val_loss: 0.5642 - val_accuracy: 0.3676\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5100 - accuracy: 0.3667 - val_loss: 0.6389 - val_accuracy: 0.3676\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5820 - accuracy: 0.3675 - val_loss: 0.5914 - val_accuracy: 0.3676\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.3832 - val_loss: 0.6648 - val_accuracy: 0.3676\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4789 - accuracy: 0.3722 - val_loss: 0.5389 - val_accuracy: 0.3676\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5450 - accuracy: 0.3625 - val_loss: 0.7010 - val_accuracy: 0.3676\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5026 - accuracy: 0.3723 - val_loss: 0.5689 - val_accuracy: 0.3676\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6197 - accuracy: 0.3724 - val_loss: 0.8027 - val_accuracy: 0.3676\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5216 - accuracy: 0.3818 - val_loss: 0.6100 - val_accuracy: 0.3676\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5002 - accuracy: 0.3923 - val_loss: 0.5588 - val_accuracy: 0.3676\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5613 - accuracy: 0.3812 - val_loss: 0.7161 - val_accuracy: 0.3676\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.8340 - accuracy: 0.3770 - val_loss: 0.5414 - val_accuracy: 0.3676\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5058 - accuracy: 0.3786 - val_loss: 0.7114 - val_accuracy: 0.3676\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5453 - accuracy: 0.3846 - val_loss: 0.5343 - val_accuracy: 0.3676\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5243 - accuracy: 0.3739 - val_loss: 0.6956 - val_accuracy: 0.3676\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6658 - accuracy: 0.3795 - val_loss: 0.6466 - val_accuracy: 0.3676\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.6387 - accuracy: 0.3678 - val_loss: 0.6297 - val_accuracy: 0.3676\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.6090 - accuracy: 0.3669 - val_loss: 0.5603 - val_accuracy: 0.3676\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5703 - accuracy: 0.3829 - val_loss: 1.0265 - val_accuracy: 0.3676\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5975 - accuracy: 0.3881 - val_loss: 0.6549 - val_accuracy: 0.3676\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5122 - accuracy: 0.3778 - val_loss: 0.5828 - val_accuracy: 0.3676\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4663 - accuracy: 0.3908 - val_loss: 1.1766 - val_accuracy: 0.3676\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6259 - accuracy: 0.3789 - val_loss: 0.6035 - val_accuracy: 0.3676\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5295 - accuracy: 0.3747 - val_loss: 0.6571 - val_accuracy: 0.3676\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6119 - accuracy: 0.3822 - val_loss: 0.5524 - val_accuracy: 0.3676\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4626 - accuracy: 0.3852 - val_loss: 0.5725 - val_accuracy: 0.3676\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4611 - accuracy: 0.3929 - val_loss: 0.5926 - val_accuracy: 0.3676\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4953 - accuracy: 0.3705 - val_loss: 0.5575 - val_accuracy: 0.3676\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5144 - accuracy: 0.3661 - val_loss: 0.5845 - val_accuracy: 0.3676\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4680 - accuracy: 0.3874 - val_loss: 0.5834 - val_accuracy: 0.3676\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4921 - accuracy: 0.3888 - val_loss: 0.5529 - val_accuracy: 0.3676\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4860 - accuracy: 0.3861 - val_loss: 0.5547 - val_accuracy: 0.3676\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4745 - accuracy: 0.4027 - val_loss: 0.5814 - val_accuracy: 0.3676\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4351 - accuracy: 0.3718 - val_loss: 0.6196 - val_accuracy: 0.3676\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5302 - accuracy: 0.3708 - val_loss: 0.7510 - val_accuracy: 0.3676\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6070 - accuracy: 0.3700 - val_loss: 0.6144 - val_accuracy: 0.3676\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4878 - accuracy: 0.3746 - val_loss: 0.6065 - val_accuracy: 0.3676\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4683 - accuracy: 0.3722 - val_loss: 0.7860 - val_accuracy: 0.3676\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5487 - accuracy: 0.3830 - val_loss: 0.6492 - val_accuracy: 0.3676\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5198 - accuracy: 0.3806 - val_loss: 0.7584 - val_accuracy: 0.3676\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5844 - accuracy: 0.3835 - val_loss: 0.6769 - val_accuracy: 0.3676\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5101 - accuracy: 0.3749 - val_loss: 0.8579 - val_accuracy: 0.3676\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.7610 - accuracy: 0.3758 - val_loss: 0.5427 - val_accuracy: 0.3676\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4389 - accuracy: 0.3787 - val_loss: 0.5423 - val_accuracy: 0.3676\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5978 - accuracy: 0.3647 - val_loss: 0.5517 - val_accuracy: 0.3676\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4433 - accuracy: 0.3744 - val_loss: 0.6061 - val_accuracy: 0.3676\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.3705 - val_loss: 0.6163 - val_accuracy: 0.3676\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4759 - accuracy: 0.3808 - val_loss: 0.6728 - val_accuracy: 0.3676\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4617 - accuracy: 0.3799 - val_loss: 0.5744 - val_accuracy: 0.3676\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5574 - accuracy: 0.3756 - val_loss: 0.8499 - val_accuracy: 0.3676\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.7570 - accuracy: 0.3664 - val_loss: 0.5934 - val_accuracy: 0.3676\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4552 - accuracy: 0.3704 - val_loss: 0.5347 - val_accuracy: 0.3676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5102 - accuracy: 0.3913 - val_loss: 0.7108 - val_accuracy: 0.3676\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6538 - accuracy: 0.3804 - val_loss: 0.5206 - val_accuracy: 0.3676\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4338 - accuracy: 0.3840 - val_loss: 0.5530 - val_accuracy: 0.3676\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4887 - accuracy: 0.3802 - val_loss: 0.5661 - val_accuracy: 0.3676\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5112 - accuracy: 0.3908 - val_loss: 0.5417 - val_accuracy: 0.3676\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4979 - accuracy: 0.3746 - val_loss: 0.8188 - val_accuracy: 0.3661\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6246 - accuracy: 0.3756 - val_loss: 0.5834 - val_accuracy: 0.3676\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5692 - accuracy: 0.3556 - val_loss: 0.5585 - val_accuracy: 0.3676\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4884 - accuracy: 0.3810 - val_loss: 0.7006 - val_accuracy: 0.3676\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.6260 - accuracy: 0.3580 - val_loss: 0.5511 - val_accuracy: 0.3676\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4349 - accuracy: 0.3791 - val_loss: 0.5720 - val_accuracy: 0.3676\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4560 - accuracy: 0.3714 - val_loss: 0.5632 - val_accuracy: 0.3676\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5429 - accuracy: 0.3759 - val_loss: 0.5261 - val_accuracy: 0.3676\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4314 - accuracy: 0.3771 - val_loss: 0.5774 - val_accuracy: 0.3676\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4763 - accuracy: 0.3887 - val_loss: 0.6231 - val_accuracy: 0.3676\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4781 - accuracy: 0.3803 - val_loss: 0.5342 - val_accuracy: 0.3676\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4479 - accuracy: 0.3772 - val_loss: 0.5644 - val_accuracy: 0.3676\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5581 - accuracy: 0.3679 - val_loss: 0.6321 - val_accuracy: 0.3676\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6203 - accuracy: 0.3772 - val_loss: 0.6945 - val_accuracy: 0.3676\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5574 - accuracy: 0.3652 - val_loss: 0.5570 - val_accuracy: 0.3676\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4725 - accuracy: 0.3782 - val_loss: 0.5673 - val_accuracy: 0.3676\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5000 - accuracy: 0.3667 - val_loss: 0.5881 - val_accuracy: 0.3676\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4935 - accuracy: 0.3623 - val_loss: 0.7276 - val_accuracy: 0.3676\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6034 - accuracy: 0.3709 - val_loss: 0.5368 - val_accuracy: 0.3676\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6160 - accuracy: 0.3916 - val_loss: 0.5373 - val_accuracy: 0.3676\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4619 - accuracy: 0.3860 - val_loss: 0.5971 - val_accuracy: 0.3676\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.36 - 0s 6ms/step - loss: 0.4841 - accuracy: 0.3653 - val_loss: 0.6309 - val_accuracy: 0.3676\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4665 - accuracy: 0.3776 - val_loss: 0.6382 - val_accuracy: 0.3676\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4702 - accuracy: 0.3827 - val_loss: 0.5570 - val_accuracy: 0.3676\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4566 - accuracy: 0.3815 - val_loss: 0.6240 - val_accuracy: 0.3676\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5327 - accuracy: 0.3728 - val_loss: 0.5403 - val_accuracy: 0.3676\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.5671 - accuracy: 0.3699 - val_loss: 0.5775 - val_accuracy: 0.3676\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4842 - accuracy: 0.3656 - val_loss: 0.5845 - val_accuracy: 0.3676\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5182 - accuracy: 0.3795 - val_loss: 0.5551 - val_accuracy: 0.3676\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5668 - accuracy: 0.3682 - val_loss: 0.5874 - val_accuracy: 0.3676\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4904 - accuracy: 0.3674 - val_loss: 0.5337 - val_accuracy: 0.3676\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4527 - accuracy: 0.3771 - val_loss: 0.5301 - val_accuracy: 0.3676\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4628 - accuracy: 0.3903 - val_loss: 0.5636 - val_accuracy: 0.3676\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4899 - accuracy: 0.3682 - val_loss: 0.5789 - val_accuracy: 0.3676\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4501 - accuracy: 0.3680 - val_loss: 0.5636 - val_accuracy: 0.3676\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4911 - accuracy: 0.3622 - val_loss: 0.5613 - val_accuracy: 0.3676\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4921 - accuracy: 0.3858 - val_loss: 0.6019 - val_accuracy: 0.3676\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.3847 - val_loss: 0.5637 - val_accuracy: 0.3661\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4955 - accuracy: 0.3669 - val_loss: 0.6363 - val_accuracy: 0.3676\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4934 - accuracy: 0.3680 - val_loss: 0.5561 - val_accuracy: 0.3676\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4588 - accuracy: 0.3861 - val_loss: 0.5963 - val_accuracy: 0.3676\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4808 - accuracy: 0.3850 - val_loss: 0.5497 - val_accuracy: 0.3676\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4752 - accuracy: 0.3675 - val_loss: 0.5580 - val_accuracy: 0.3676\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4288 - accuracy: 0.3785 - val_loss: 0.5142 - val_accuracy: 0.3676\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4601 - accuracy: 0.3834 - val_loss: 0.9587 - val_accuracy: 0.3676\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5129 - accuracy: 0.3832 - val_loss: 0.5524 - val_accuracy: 0.3676\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4669 - accuracy: 0.3809 - val_loss: 0.5869 - val_accuracy: 0.3676\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4215 - accuracy: 0.3894 - val_loss: 0.5611 - val_accuracy: 0.3676\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4757 - accuracy: 0.3819 - val_loss: 0.5706 - val_accuracy: 0.3676\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5657 - accuracy: 0.3864 - val_loss: 0.6807 - val_accuracy: 0.3676\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4911 - accuracy: 0.3706 - val_loss: 0.5707 - val_accuracy: 0.3676\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4572 - accuracy: 0.3769 - val_loss: 0.5621 - val_accuracy: 0.3676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5020 - accuracy: 0.3713 - val_loss: 0.6309 - val_accuracy: 0.3676\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5281 - accuracy: 0.3841 - val_loss: 0.5921 - val_accuracy: 0.3676\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4796 - accuracy: 0.3889 - val_loss: 0.5610 - val_accuracy: 0.3676\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4494 - accuracy: 0.3924 - val_loss: 0.5197 - val_accuracy: 0.3676\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4318 - accuracy: 0.3781 - val_loss: 0.7131 - val_accuracy: 0.3676\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.4851 - accuracy: 0.3816 - val_loss: 0.6370 - val_accuracy: 0.3676\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5718 - accuracy: 0.3839 - val_loss: 0.6201 - val_accuracy: 0.3676\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4747 - accuracy: 0.3759 - val_loss: 0.5541 - val_accuracy: 0.3676\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4917 - accuracy: 0.3629 - val_loss: 0.5887 - val_accuracy: 0.3676\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5119 - accuracy: 0.3730 - val_loss: 0.5358 - val_accuracy: 0.3676\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5157 - accuracy: 0.3603 - val_loss: 0.5848 - val_accuracy: 0.3676\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4727 - accuracy: 0.3710 - val_loss: 0.5335 - val_accuracy: 0.3676\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4903 - accuracy: 0.3739 - val_loss: 0.5486 - val_accuracy: 0.3676\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5675 - accuracy: 0.3734 - val_loss: 0.5683 - val_accuracy: 0.3676\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6139 - accuracy: 0.3613 - val_loss: 0.5737 - val_accuracy: 0.3676\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.9401 - accuracy: 0.3749 - val_loss: 1.0503 - val_accuracy: 0.3676\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5071 - accuracy: 0.3882 - val_loss: 0.6086 - val_accuracy: 0.3676\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.4943 - accuracy: 0.3738 - val_loss: 0.5642 - val_accuracy: 0.3676\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4678 - accuracy: 0.3677 - val_loss: 0.5302 - val_accuracy: 0.3676\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5210 - accuracy: 0.3763 - val_loss: 0.5841 - val_accuracy: 0.3676\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4722 - accuracy: 0.3817 - val_loss: 0.7725 - val_accuracy: 0.3676\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4524 - accuracy: 0.3855 - val_loss: 0.5379 - val_accuracy: 0.3676\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4427 - accuracy: 0.3732 - val_loss: 0.6440 - val_accuracy: 0.3676\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4802 - accuracy: 0.3779 - val_loss: 0.5411 - val_accuracy: 0.3676\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6035 - accuracy: 0.3790 - val_loss: 0.5911 - val_accuracy: 0.3676\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.5104 - accuracy: 0.3945 - val_loss: 0.5424 - val_accuracy: 0.3676\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.6347 - accuracy: 0.3691 - val_loss: 0.5412 - val_accuracy: 0.3676\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 0.5511 - accuracy: 0.3835 - val_loss: 0.5450 - val_accuracy: 0.3676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1268c33fa00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=5, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=200, batch_size=64, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:00<00:00, 16.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# testing the neural network\n",
    "# Generating a test dataset\n",
    "\n",
    "nb_sample=1000\n",
    "data= []\n",
    "for n in tqdm(range(nb_sample)):\n",
    "    sigma= np.random.uniform(low=volatilities[0],high=volatilities[-1])\n",
    "    r = np.random.uniform(low=interest_rates[0],high=interest_rates[-1])\n",
    "    T= np.random.uniform(low=expiry_times[0],high=expiry_times[-1])\n",
    "    S0= np.random.uniform(low=spot_prices[0],high=spot_prices[-1])\n",
    "    K= np.random.uniform(low=(1-0.2)*S0,high=(1+0.2)*S0)\n",
    "    price_paths=[] \n",
    "    for i in range(0, paths):\n",
    "        price_paths.append(HestonPricer(mu, teta, k, volatility_std, \n",
    "                                        S0, sigma, dt, T/252).prices)\n",
    "    #price = analytical_call_pricer(r, sigma, S0, T,K,option_type)\n",
    "    call_payoffs = []\n",
    "    ec = European_Call_Payoff(K)\n",
    "\n",
    "    for price_path in price_paths:\n",
    "        call_payoffs.append(ec.get_payoff(price_path[-1])/((1 + r)**(T/252)))  # We get the last stock price in the series generated by GBM to determin the payoff and discount it by one year \n",
    "    data.append({\n",
    "                        'volatility':sigma,\n",
    "                        'interest_rate':r,\n",
    "                        'expiry_time':T,\n",
    "                        'initial_price':S0,\n",
    "                        'strike_price':K,\n",
    "                        'price':np.average(call_payoffs)\n",
    "                    })\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()\n",
    "X_test=df[['volatility','interest_rate','expiry_time','initial_price','strike_price']].values\n",
    "y_test= df[['price']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6178 - accuracy: 0.0090\n",
      "test loss, test acc: 0.6178281903266907\n"
     ]
    }
   ],
   "source": [
    "loss, _ = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
